# ============================================
# Papr Memory - Open Source Configuration
# ============================================
#
# SETUP INSTRUCTIONS:
#
#   1. Copy this file:
#        cp .env.example .env.opensource
#
#   2. Edit .env.opensource with your REAL API keys:
#        - OPENAI_API_KEY       (required for LLM operations)
#        - GROQ_API_KEY         (optional, for fast LLM)
#        - DEEPINFRA_TOKEN      (only if USE_LOCAL_EMBEDDINGS=false)
#
#   3. Start services:
#        docker compose up -d
#
#   4. On first startup, the system auto-creates:
#        - Parse schemas
#        - Default user + workspace
#        - API key  →  saved to .env.generated
#        - Test credentials (TEST_* vars)  →  appended to .env.opensource
#
#   5. Your API key is in .env.generated (or docker logs papr-memory)
#
# FILES:
#   .env.example     →  Template (committed to git). DO NOT put secrets here.
#   .env.opensource   →  Your real config (gitignored). Used by docker-compose.
#   .env             →  Alternative config name (gitignored). Also works.
#   .env.generated   →  Auto-created on first run with your API key (gitignored).
#
# ============================================

# ============================================
# Edition Configuration
# ============================================
# Set to "opensource" for self-hosted deployment
# Set to "cloud" if you're running the cloud edition
PAPR_EDITION=opensource

# ============================================
# API Key (For Testing)
# ============================================
# Auto-generated on first startup. Check .env.generated after docker compose up.
# Or generate manually:
#   python scripts/generate_api_key.py --email your@email.com --name "My Project"
PAPR_API_KEY=pmem_oss_default_testing_key_CHANGE_ME

# ============================================
# Server Configuration
# ============================================
PORT=5001
DEBUG=true
ENVIRONMENT=development
PYTHONPATH=/app
USE_DOTENV=true

# Secret key for session encryption (generate a secure random string)
# You can generate one with: python -c "import secrets; print(secrets.token_urlsafe(32))"
APP_SECRET_KEY=your-secret-key-change-me

# ============================================
# AI/LLM Configuration [REQUIRED]
# ============================================
# OpenAI API key for embeddings and LLM operations
OPENAI_API_KEY=sk-your-openai-api-key
OPENAI_ORGANIZATION=org-your-org-id

# LLM Model Configuration
LLM_MODEL=gpt-5-nano
LLM_MODEL_MINI=gpt-5-mini
LLM_MODEL_NANO=gpt-5-nano

# ============================================
# MongoDB Configuration [REQUIRED]
# ============================================
# For Docker: mongodb://admin:password@mongodb:27017/papr_memory?authSource=admin
# For local: mongodb://localhost:27017/papr_memory
MONGO_URI=mongodb://admin:password@mongodb:27017/papr_memory?authSource=admin

# MongoDB credentials (used by docker-compose)
MONGODB_USERNAME=admin
MONGODB_PASSWORD=password
MONGODB_DATABASE=papr_memory

# ============================================
# Redis Configuration [REQUIRED]
# ============================================
# For Docker: redis://:password@redis:6379/0
# For local: redis://localhost:6379/0
REDIS_URL=redis://:password@redis:6379/0
REDIS_PASSWORD=password

# ============================================
# Neo4j Configuration [REQUIRED]
# ============================================
# For Docker: bolt://neo4j:7687
# For local: bolt://localhost:7687
NEO4J_URL=bolt://neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_SECRET=password

# ============================================
# Qdrant Vector Database [REQUIRED]
# ============================================
# For Docker: http://qdrant:6333
# For local: http://localhost:6333
QDRANT_URL=http://qdrant:6333
# Leave empty for self-hosted Qdrant without auth
QDRANT_API_KEY=

# Qdrant collection names (auto-selected based on embedding dimensions)
# - Qwen0pt6B: 1024 dimensions (used with Qwen3-Embedding-0.6B)
# - Qwen4B: 2560 dimensions (used with Qwen3-Embedding-4B)
# The system automatically uses the correct collection based on LOCAL_EMBEDDING_DIMENSIONS
QDRANT_COLLECTION_QWEN0pt6B=Qwen0pt6B
QDRANT_COLLECTION_QWEN4B=Qwen4B

# ============================================
# Parse Server Configuration [REQUIRED]
# ============================================
# Parse Server is used for user management and ACL
# For Docker: http://parse-server:1337
# For local: http://localhost:1337
# NOTE: Do NOT include /parse in the URL - the code appends /parse to all API paths
PARSE_SERVER_URL=http://parse-server:1337

# Parse credentials (generate random UUIDs for security in production)
PARSE_APPLICATION_ID=papr-oss-app-id
PARSE_REST_API_KEY=papr-oss-rest-key
PARSE_MASTER_KEY=papr-oss-master-key

# Docker compose uses these variable names
PARSE_SERVER_APPLICATION_ID=papr-oss-app-id
PARSE_SERVER_MASTER_KEY=papr-oss-master-key

# Parse Dashboard credentials (optional, for development only)
# Dashboard URL: http://localhost:4040
PARSE_DASHBOARD_USER=admin
PARSE_DASHBOARD_PASSWORD=password
PARSE_DASHBOARD_SESSION_SECRET=your-dashboard-secret

# ============================================
# Public URLs (for OAuth callbacks, etc.)
# ============================================
# Your public-facing server URL
PUBLIC_SERVER_URL=http://localhost:5001
PYTHON_SERVER_URL=http://localhost:5001
WEB_APP_URL=http://localhost:3000

# ============================================
# Telemetry Configuration (Optional)
# ============================================
# We collect anonymous usage data to improve Papr Memory
# See docs/TELEMETRY.md for what we collect
# Set to "false" to opt out completely
TELEMETRY_ENABLED=true

# PostHog is used for self-hostable analytics
# You can self-host PostHog or use the cloud version
# Leave empty to disable
POSTHOG_API_KEY=
POSTHOG_HOST=https://app.posthog.com

# Amplitude - MUST be empty for open source (cloud-only)
# Note: Some legacy code still references AMPLITUDE_API_KEY
# Leaving it empty prevents Amplitude from initializing
AMPLITUDE_API_KEY=

# ============================================
# Embedding Configuration
# ============================================
# Local Embeddings (RECOMMENDED for Open Source)
# By default, we use local Qwen3-Embedding-0.6B model for embeddings
# This runs entirely on your device without external API calls
# 
# Set to "true" to use local embedding models (default for open source)
# Set to "false" to use cloud embedding APIs (requires API tokens below)
USE_LOCAL_EMBEDDINGS=true

# Local embedding model configuration
# ⚠️ IMPORTANT: Choose ONE of the following models:
#
# Option 1: Qwen3-Embedding-0.6B (RECOMMENDED for most users)
#   - Smaller, faster model (~1.2GB download)
#   - 1024 dimensions, 32k context
#   - Good balance of speed and quality
LOCAL_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
LOCAL_EMBEDDING_DIMENSIONS=1024
#
# Option 2: Qwen3-Embedding-4B (for higher quality)
#   - Larger, more accurate model (~8GB download)
#   - 2560 dimensions, 32k context
#   - Better embedding quality but slower
# LOCAL_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-4B
# LOCAL_EMBEDDING_DIMENSIONS=2560
#
# ⚠️ NOTE: Qdrant collections are created per dimension size.
# If you change models, you may need to:
#   1. Delete old collection: curl -X DELETE http://localhost:6333/collections/Qwen0pt6B
#   2. Restart services: docker compose restart
#   3. Or use: docker compose down -v && docker compose up -d (clears all data)

# Cloud Embedding APIs (Optional - only needed if USE_LOCAL_EMBEDDINGS=false)
# Hugging Face for alternative embeddings
HUGGING_FACE_ACCESS_TOKEN=

# DeepInfra for cloud-based Qwen embeddings (faster but requires API key)
DEEPINFRA_TOKEN=
DEEPINFRA_API_URL=https://api.deepinfra.com/v1/openai/embeddings

# ============================================
# Content Limits
# ============================================
# Maximum content length in bytes (default: 600KB)
MAX_CONTENT_LENGTH=600000
MAX_BATCH_SIZE=50
MAX_FILE_SIZE_BYTES=52428800

# Token limits
MAX_TOKENS_PER_ITEM=4000
MAX_TOTAL_BATCH_TOKENS=100000

# ============================================
# Logging Configuration
# ============================================
LOGGING_ENV=development
LoggingtoFile=true

# ============================================
# Docker Image Configuration
# ============================================
# Used by docker-compose for building
IMAGE_NAME=papr_memory_oss
IMAGE_TAG=latest

# ============================================
# SSL Configuration (Optional)
# ============================================
# Path to CA certificates bundle
# Linux: /etc/ssl/certs/ca-certificates.crt
# macOS: /opt/homebrew/etc/openssl@3/cert.pem
# SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt
# REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt

# ============================================
# Feature Flags (Open Source defaults)
# ============================================
# These are already configured in config/opensource.yaml
# Override here if needed

# Enable schema selection via LLM
ENABLE_LLM_SCHEMA_SELECTION=true
OPENAI_SCHEMA_SELECTOR_MODEL=gpt-5-nano

# Enable hierarchical document chunking
FEATURE_HIERARCHICAL_CHUNKING=true

# ============================================
# Temporal Configuration (Durable Workflows)
# ============================================
# Temporal enables durable workflows for batch memory processing and
# document ingestion. For self-hosted deployments, use the Docker Temporal.
#
# For Docker (self-hosted Temporal):
TEMPORAL_ADDRESS=temporal:7233
TEMPORAL_NAMESPACE=default
TEMPORAL_TASK_QUEUE=memory-processing

# For Temporal Cloud (optional - if using Temporal Cloud instead):
# TEMPORAL_ADDRESS=your-namespace.tmprl.cloud:7233
# TEMPORAL_NAMESPACE=your-namespace
# TEMPORAL_API_KEY=your-temporal-cloud-api-key

# Enable/disable Temporal (set to false to use simple background tasks instead)
TEMPORAL_ENABLED=true

# ============================================
# Document Processing Configuration (Optional)
# ============================================
# These providers are optional - configure as needed for document ingestion.
# Leave as placeholders if you don't need document processing.

# TensorLake - AI document parsing
TENSORLAKE_API_KEY=your-tensorlake-api-key-here
TENSORLAKE_BASE_URL=https://api.tensorlake.ai

# Reducto - Document structure extraction
REDUCTO_API_KEY=your-reducto-api-key-here
REDUCTO_ENVIRONMENT=production
REDUCTO_PIPELINE_ID=your-reducto-pipeline-id-here

# Google Gemini - Vision/PDF processing
GOOGLE_API_KEY=your-google-api-key-here

# Groq - Fast LLM for document analysis
GROQ_API_KEY=your-groq-api-key-here
GROQ_NEO_CYPHER=1
GROQ_FALLBACK_MODEL=openai/gpt-oss-20b

# Cohere - Reranking (optional)
COHERE_API_KEY=your-cohere-api-key-here

# ============================================
# Test Credentials (Auto-Generated)
# ============================================
# These are auto-populated on first startup by the bootstrap script.
# After running `docker compose up -d`, check .env.opensource for real values.
# Do NOT set these manually - they are created from Parse Server data.
#
# TEST_SESSION_TOKEN=r:auto-generated-on-first-run
# TEST_X_USER_API_KEY=pmem_oss_auto-generated-on-first-run
# TEST_USER_ID=auto-generated
# TEST_TENANT_ID=auto-generated
# TEST_WORKSPACE_ID=auto-generated
# TEST_NAMESPACE_ID=auto-generated
# TEST_ORGANIZATION_ID=auto-generated

# ============================================
# CLOUD-ONLY FEATURES (Not needed for OSS)
# ============================================
# The following are only used in cloud edition:
# - AUTH0_* (OAuth authentication)
# - STRIPE_* (Payment processing)
# - AMPLITUDE_* (Cloud analytics)
# - AZURE_* (Azure services)
# - NEO4J_GRAPHQL_ENDPOINT (Neo4j Aura GraphQL)
